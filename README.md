# aws-glue-template
This repository contains libraries used in the [AWS Glue](https://aws.amazon.com/glue) service. These libraries extend [Apache Spark](https://spark.apache.org/) with additional data types and operations for ETL workflows. They are used in code generated by the AWS Glue service and can be used in scripts submitted with Glue jobs. 

## Content

- [awsglue](awsglue) -- This Python package includes the Python interfaces to the AWS Glue ETL library.

## Environment Setup
To install the dependencies required by AWS Glue (Maven, Spark, and Hadoop's winutils), run:

```
bin/install-spark.ps1 <install path>
```

`<install path>` is the path where Maven, Spark, and Hadoop's winutils will be installed. If any applications are already installed, they will be skipped automatically.

For more information on what the environment setup is doing, see: [Environment Setup](docs/environment-setup.md)

## Build AWS Glue Libs
To build the AWS Glue Libs, run:

```
bin/build-aws-glue-libs.ps1
```

This will use Maven to download the dependent jar files from a S3 backed maven repository. Then `conf/spark-defaults.conf` is created which Spark will use to load the Glue ETL jars.

## Running jobs locally

### Using Visual Studio Code
Open the python job and press F5. The launch config in `.vscode/launch.json` is set up to enable debugging through Spark's `spark-submit` script. VS Code's debugging features can be used, including: breakpoints, inspecting variables, etc.

### Running glue-pyspark shell, glue-spark-submit and pytest locally
Glue submit: `./bin/glue-spark-submit.cmd <job.py path> <additional spark-submit arguments>`  
Glue shell: `./bin/glue-pyspark.cmd`  
pytest: `./bin/glue-pytest.cmd <test.py path> <additional pytest arguments>`  

## Licensing
The libraries in this repository licensed under the [Amazon Software License](http://aws.amazon.com/asl/) (the "License"). They may not be used except in compliance with the License, a copy of which is included here in the LICENSE file.
